"""
Strategic Data Collection Test - Simplified

Tests strategic prompt crafting with key indicators to collect 
high-quality cybersecurity AI impact articles using Perplexity API.
"""

import asyncio
from datetime import datetime
from aih.gather.perplexity import PerplexityConnector
from aih.config import settings

async def test_strategic_prompts():
    """Test strategic data collection with key indicators."""
    
    print("ğŸš€ Strategic Data Collection Test")
    print("=" * 50)
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"API Key: {settings.perplexity_api_key[:15]}..." if settings.perplexity_api_key else "âŒ No API Key")
    print()
    
    try:
        # Initialize Perplexity connector
        connector = PerplexityConnector()
        print("âœ… Perplexity connector initialized")
        
        # Strategic query with targeted approach
        strategic_query = """
        CYBERSECURITY AI WORKFORCE IMPACT RESEARCH 2024-2025:
        
        Find recent authoritative sources discussing AI automation REPLACING cybersecurity professionals:
        
        KEY INDICATORS TO SEARCH FOR:
        â€¢ SIEM/log analysis becoming fully automated (no human analysts needed)
        â€¢ AI threat detection systems replacing SOC analysts  
        â€¢ Autonomous incident response eliminating human responders
        â€¢ Job displacement studies in cybersecurity field
        â€¢ "Lights-out" or "human-free" security operations centers
        â€¢ Workforce reduction due to AI automation
        
        PRIORITIZE CREDIBLE SOURCES:
        â€¢ Academic research papers and studies
        â€¢ Gartner, Forrester, McKinsey workforce reports
        â€¢ SANS Institute cybersecurity surveys
        â€¢ Government/NIST cybersecurity workforce analysis
        â€¢ Major security vendor research (IBM, Microsoft, Palo Alto)
        â€¢ Peer-reviewed cybersecurity journals
        
        FOCUS TIMEFRAME: 2024-2025 developments and future predictions
        """
        
        print("ğŸ“¡ Strategic Query Focus:")
        print("   ğŸ¯ Category: AI REPLACING cybersecurity jobs")
        print("   ğŸ“Š Sources: Academic, industry reports, government studies")
        print("   ğŸ“… Timeframe: 2024-2025")
        print("   ğŸ” Key Indicators: automation, job displacement, autonomous systems")
        print()
        
        # Collect artifacts using strategic approach
        print("ğŸ” Collecting articles with strategic prompting...")
        artifacts = await connector.collect(
            query=strategic_query,
            max_results=4,  # Test with small batch
            category="replace",
            timeframe="2024"
        )
        
        print(f"âœ… Successfully collected {len(artifacts)} strategic artifacts")
        print()
        
        # Analyze collected results
        print("ğŸ“Š STRATEGIC COLLECTION ANALYSIS")
        print("=" * 50)
        
        for i, artifact in enumerate(artifacts, 1):
            print(f"\nğŸ“„ Artifact {i}: Strategic Analysis")
            print("-" * 30)
            print(f"ğŸ“ Title: {artifact.title}")
            print(f"ğŸ”— Source URL: {artifact.url}")
            print(f"ğŸ“… Collection Time: {artifact.collected_at}")
            print(f"ğŸ·ï¸  Target Category: replace")
            
            # Analyze source credibility
            if artifact.url:
                domain = artifact.url.split('/')[2] if len(artifact.url.split('/')) > 2 else "unknown"
                print(f"ğŸ¢ Source Domain: {domain}")
                
                # Check if it's a high-credibility source
                credible_domains = [
                    'gartner.com', 'forrester.com', 'mckinsey.com', 'sans.org', 
                    'nist.gov', 'ieee.org', 'acm.org', 'arxiv.org', 'researchgate.net',
                    'ibm.com', 'microsoft.com', 'cisco.com', 'paloaltonetworks.com'
                ]
                
                is_credible = any(cd in domain.lower() for cd in credible_domains)
                print(f"ğŸ“‹ High-Credibility Source: {'âœ… Yes' if is_credible else 'â“ Unknown'}")
            
            # Analyze content for key indicators
            content = artifact.content.lower()
            key_indicators = [
                "replacing", "automation", "autonomous", "job displacement", 
                "workforce reduction", "siem", "soc", "threat hunter", 
                "analyst", "lights-out", "human-free"
            ]
            
            found_indicators = [ind for ind in key_indicators if ind in content]
            print(f"ğŸ¯ Key Indicators Found ({len(found_indicators)}/{len(key_indicators)}): {', '.join(found_indicators)}")
            
            # Content quality assessment
            content_length = len(artifact.content)
            print(f"ğŸ“ Content Length: {content_length} characters")
            
            # Show content preview
            preview = artifact.content[:300] + "..." if len(artifact.content) > 300 else artifact.content
            print(f"ğŸ“– Content Preview: {preview}")
            
        # Summary statistics
        print(f"\nğŸ“ˆ STRATEGIC COLLECTION SUMMARY")
        print("=" * 50)
        print(f"Total artifacts collected: {len(artifacts)}")
        
        if artifacts:
            # Count high-credibility sources
            credible_count = 0
            total_indicators = 0
            
            for artifact in artifacts:
                if artifact.url:
                    domain = artifact.url.split('/')[2] if len(artifact.url.split('/')) > 2 else ""
                    credible_domains = ['gartner.com', 'forrester.com', 'mckinsey.com', 'sans.org', 'nist.gov']
                    if any(cd in domain.lower() for cd in credible_domains):
                        credible_count += 1
                
                content = artifact.content.lower()
                key_indicators = ["replacing", "automation", "autonomous", "job displacement"]
                found = sum(1 for ind in key_indicators if ind in content)
                total_indicators += found
            
            print(f"High-credibility sources: {credible_count}/{len(artifacts)}")
            print(f"Average key indicators per artifact: {total_indicators/len(artifacts):.1f}")
            print(f"Strategic targeting effectiveness: {'ğŸ¯ High' if total_indicators > len(artifacts) else 'ğŸ“Š Moderate'}")
        
        print("\nğŸ‰ Strategic data collection test completed successfully!")
        print("âœ… Perplexity API integration working")
        print("âœ… Strategic prompting with key indicators functional")
        print("âœ… Quality source targeting operational")
        print("âœ… Ready for full pipeline implementation")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_strategic_prompts()) 