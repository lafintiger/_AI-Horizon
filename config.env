# AI-Horizon Local Models Configuration
# Copy this to config.env and configure for your setup

# ================================
# LOCAL MODEL CONFIGURATION
# ================================

# Enable local models (set to false to use external APIs)
USE_LOCAL_MODELS=true

# Local service URLs
PERPLEXICA_URL=http://localhost:3000
OLLAMA_URL=http://localhost:11434

# Model assignments for different tasks
# Optimized for your high-quality model collection
LOCAL_CLASSIFICATION_MODEL=llama3:latest
LOCAL_WISDOM_MODEL=qwen3:32b-q8_0
LOCAL_CHAT_MODEL=mistral-nemo:12b-instruct-2407-q6_K
LOCAL_ANALYSIS_MODEL=llama3.3:70b-instruct-q5_K_M
LOCAL_CODING_MODEL=qwen2.5-coder:32b-instruct-fp16
LOCAL_HEAVY_ANALYSIS_MODEL=nemotron:70b-instruct-q5_K_M

# Alternative high-performance options (uncomment if you have sufficient resources):
# LOCAL_ANALYSIS_MODEL=qwen3:32b-q8_0  # Very powerful but needs 32GB+ RAM
# LOCAL_ANALYSIS_MODEL=llama3.3:70b-instruct-q5_K_M  # Top-tier but needs 50GB+ RAM

# Fallback to external APIs if local models fail
FALLBACK_TO_EXTERNAL_APIS=false

# ================================
# EXTERNAL API KEYS (Optional/Fallback)
# ================================

# Only needed if USE_LOCAL_MODELS=false or FALLBACK_TO_EXTERNAL_APIS=true
PERPLEXITY_API_KEY=
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# ================================
# APPLICATION SETTINGS
# ================================

# Database
DATABASE_URL=sqlite:///data/aih_database.db

# Logging
LOG_LEVEL=INFO
DEBUG=false

# Data Collection
DEFAULT_MAX_ARTIFACTS_PER_RUN=50
DEFAULT_SEARCH_LOOKBACK_DAYS=7

# Classification
DEFAULT_CONFIDENCE_THRESHOLD=0.6
DEFAULT_LLM_MODEL=llama3:latest

# Rate Limiting (generous for local models)
MAX_API_CALLS_PER_MINUTE=10
PERPLEXITY_REQUESTS_PER_MINUTE=60
LOCAL_MODEL_REQUESTS_PER_MINUTE=1000

# File Paths
DATA_DIR=./data
LOGS_DIR=./logs
REPORTS_DIR=./data/reports

# ================================
# YOUR AVAILABLE MODELS
# ================================

# Fast & Efficient (4-5GB RAM):
# - llama3:latest
# - mistral:latest
# - qwen3:latest

# Balanced Performance (10-12GB RAM):
# - mistral-nemo:12b-instruct-2407-q6_K

# High Performance (25-35GB RAM):
# - qwen3:32b-q8_0
# - gemma3:27b-it-q8_0

# Maximum Performance (45-50GB RAM):
# - llama3.3:70b-instruct-q5_K_M
# - nemotron:70b-instruct-q5_K_M
# - cogito:70b

# Specialized Models:
# - meditron:70b-q5_1 (medical domain)
# - qwen2.5-coder:32b-instruct-fp16 (coding tasks)
# - aya-expanse:32b-fp16 (multilingual)

# ================================
# DOCKER SETUP COMMANDS
# ================================

# 1. Start Perplexica:
# git clone https://github.com/ItzCrazyKns/Perplexica.git
# cd Perplexica && docker compose up -d

# 2. Ollama is already running with your models!

# 3. Test setup:
# python test_local_models.py 