{% extends "base.html" %}

{% block title %}Methodology - AI-Horizon{% endblock %}

{% block extra_styles %}
<style>
    .methodology-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    
    .methodology-header {
        text-align: center;
        margin-bottom: 40px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 40px;
        border-radius: 10px;
    }
    
    .methodology-section {
        background: white;
        border-radius: 8px;
        padding: 30px;
        margin-bottom: 30px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    
    .methodology-section h2 {
        color: #2c3e50;
        border-bottom: 3px solid #3498db;
        padding-bottom: 10px;
        margin-bottom: 20px;
    }
    
    .methodology-section h3 {
        color: #34495e;
        margin-top: 25px;
        margin-bottom: 15px;
    }
    
    .formula-box {
        background: #f8f9fa;
        border: 2px solid #e9ecef;
        border-radius: 5px;
        padding: 15px;
        margin: 15px 0;
        font-family: 'Courier New', monospace;
        overflow-x: auto;
    }
    
    .formula-title {
        font-weight: bold;
        color: #495057;
        margin-bottom: 8px;
    }
    
    .confidence-table {
        width: 100%;
        border-collapse: collapse;
        margin: 15px 0;
    }
    
    .confidence-table th,
    .confidence-table td {
        border: 1px solid #dee2e6;
        padding: 8px 12px;
        text-align: left;
    }
    
    .confidence-table th {
        background-color: #f8f9fa;
        font-weight: bold;
    }
    
    .threshold-list {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 15px;
        margin: 15px 0;
    }
    
    .threshold-list h4 {
        margin-top: 0;
        color: #856404;
    }
    
    .warning-box {
        background: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 15px 0;
        color: #721c24;
    }
    
    .data-sources {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 15px 0;
    }
    
    .navigation-links {
        text-align: center;
        margin: 30px 0;
    }
    
    .nav-button {
        display: inline-block;
        padding: 10px 20px;
        margin: 0 10px;
        background: #3498db;
        color: white;
        text-decoration: none;
        border-radius: 5px;
        transition: background 0.3s;
    }
    
    .nav-button:hover {
        background: #2980b9;
    }
</style>
{% endblock %}

{% block content %}
    <div class="methodology-container">
        <div class="methodology-header">
            <h1>üî¨ AI-Horizon Analysis Methodology</h1>
            <p>Understanding the Science Behind Our Intelligence Platform</p>
            <p><em>Transparent, reproducible methods for cybersecurity workforce analysis</em></p>
        </div>

        <div class="navigation-links">
            <a href="/" class="nav-button">üè† Home</a>
            <a href="/analysis" class="nav-button">üìä Analysis</a>
            <a href="/summaries" class="nav-button">üìã Reports</a>
        </div>

        <!-- Data Collection Methodology -->
        <div class="methodology-section" id="data-collection">
            <h2>üåê Data Collection & Processing</h2>
            
            <h3>Automated Collection System</h3>
            <p>Our system uses the Perplexity API for real-time data collection with sophisticated query generation and duplicate detection.</p>
            
            <div class="formula-box">
                <div class="formula-title">Collection Query Generation:</div>
                <code>
                base_query = "cybersecurity workforce artificial intelligence impact"
                <br>
                For each category (replace, augment, new_tasks, human_only):
                <br>‚Ä¢ category_keywords = get_category_keywords(category)
                <br>‚Ä¢ enhanced_query = base_query + " " + category_keywords.join(" ")
                <br>‚Ä¢ temporal_filter = enhanced_query + " 2024-2025"
                <br><br>
                Query Examples:
                <br>‚Ä¢ REPLACE: "...AI deployment reducing headcount layoffs automation 2024"
                <br>‚Ä¢ AUGMENT: "...AI-human collaboration workforce enhancement tools 2024"
                <br>‚Ä¢ NEW_TASKS: "...emerging cybersecurity roles AI specialist skills 2024"
                </code>
            </div>
            
            <h3>Duplicate Detection Algorithm</h3>
            <div class="formula-box">
                <div class="formula-title">URL-Based Deduplication:</div>
                <code>
                For each collected article:
                <br>1. normalize_url = clean_tracking_params(article_url)
                <br>2. url_hash = sha256(normalize_url)
                <br>3. if url_hash in database: SKIP (duplicate)
                <br>4. else: process_and_store(article)
                <br><br>
                URL Normalization Rules:
                <br>‚Ä¢ Remove tracking parameters (utm_*, ref=, etc.)
                <br>‚Ä¢ Convert to lowercase
                <br>‚Ä¢ Remove trailing slashes
                <br>‚Ä¢ Standardize protocol (https://)
                </code>
            </div>
            
            <h3>Collection Rate Management</h3>
            <div class="formula-box">
                <div class="formula-title">Rate Limiting & Throttling:</div>
                <code>
                Perplexity API Limits:
                <br>‚Ä¢ queries_per_minute = 60 (enforced by API)
                <br>‚Ä¢ requests_per_hour = 3600 (enforced by API)
                <br><br>
                Collection Schedule:
                <br>‚Ä¢ base_queries = 4 categories √ó 5 queries_per_category = 20 queries/run
                <br>‚Ä¢ run_frequency = daily (configurable)
                <br>‚Ä¢ articles_per_query = ~3 (varies by API response)
                <br>‚Ä¢ expected_new_articles = 20-60 per day (after deduplication)
                </code>
            </div>
            
            <h3>Content Processing Pipeline</h3>
            <div class="formula-box">
                <div class="formula-title">Processing Workflow:</div>
                <code>
                For each collected article:
                <br>1. raw_content = extract_text_from_url(article_url)
                <br>2. metadata = extract_metadata(title, url, date, source)
                <br>3. quality_score = calculate_quality_score(content, metadata)
                <br>4. ai_categories = classify_ai_impact(content) [LLM call]
                <br>5. wisdom_extraction = extract_wisdom(content) [LLM call]
                <br>6. store_in_database(processed_article)
                <br><br>
                Error Handling:
                <br>‚Ä¢ URL access failures: retry 3x with exponential backoff
                <br>‚Ä¢ LLM API failures: queue for later processing
                <br>‚Ä¢ Invalid content: log and skip with error flag
                </code>
            </div>
            
            <h3>Source Diversity Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Source Distribution Metrics:</div>
                <code>
                source_counts = count_articles_per_domain(all_articles)
                <br>
                Shannon_Diversity = -Œ£(p[i] √ó log(p[i])) where p[i] = source_count[i]/total
                <br>Simpson_Diversity = 1 - Œ£(p[i]¬≤)
                <br><br>
                Source Health Score = (Shannon_Diversity √ó 0.6) + (Simpson_Diversity √ó 0.4)
                <br><br>
                Interpretation:
                <br>‚Ä¢ High diversity (>0.8): Good variety of sources
                <br>‚Ä¢ Medium diversity (0.5-0.8): Moderate source spread  
                <br>‚Ä¢ Low diversity (<0.5): Dominated by few sources
                </code>
            </div>
            
            <div class="data-sources">
                <h4>üìä Primary Data Sources</h4>
                <p>Data is collected from:</p>
                <ul>
                    <li><strong>Perplexity API:</strong> Real-time web search with citation tracking</li>
                    <li><strong>Source Types:</strong> News articles, industry reports, academic papers, blogs</li>
                    <li><strong>Temporal Focus:</strong> 2024-2025 publications (configurable lookback period)</li>
                    <li><strong>Geographic Scope:</strong> Global English-language sources</li>
                    <li><strong>Domain Coverage:</strong> Cybersecurity, technology, workforce development</li>
                </ul>
            </div>
            
            <div class="warning-box">
                <h4>‚ö†Ô∏è Collection Limitations</h4>
                <ul>
                    <li><strong>Language Bias:</strong> English-only sources may miss regional perspectives</li>
                    <li><strong>Search Algorithm:</strong> Results depend on Perplexity's proprietary ranking</li>
                    <li><strong>Temporal Lag:</strong> New publications may take time to appear in search results</li>
                    <li><strong>Paywall Content:</strong> Premium/subscription content may be inaccessible</li>
                    <li><strong>Source Bias:</strong> Algorithm may favor certain domains or publication types</li>
                </ul>
            </div>
        </div>

        <!-- Quality Analysis Methodology -->
        <div class="methodology-section" id="quality-analysis">
            <h2>üìà Quality Assessment Systems</h2>
            
            <div class="important-note">
                <strong>Note:</strong> AI-Horizon uses two distinct quality scoring systems for different purposes:
                <ul>
                    <li><strong>Content Quality Score:</strong> Basic validation during content ingestion</li>
                    <li><strong>Intelligence Quality Score:</strong> Strategic analysis using NATO Intelligence Doctrine</li>
                </ul>
            </div>
            
            <h3>üîµ Content Quality Score (Processing Pipeline)</h3>
            <p>Used during initial content processing to validate basic content quality and structure.</p>
            
            <h4>Content Quality Algorithm</h4>
            <div class="formula-box">
                <div class="formula-title">Content Quality Score Formula:</div>
                <code>
                Content Quality = (Length_Score √ó 0.4) + (Tech_Density √ó 0.3) + (Source_Bonus √ó 0.3)
                <br><br>
                Where:
                <br>‚Ä¢ Length_Score = Optimal range 300-3000 words (1.0 = perfect, scaled down for outliers)
                <br>‚Ä¢ Tech_Density = Cybersecurity keyword density √ó 2 (capped at 1.0)
                <br>‚Ä¢ Source_Bonus = 0.3 for academic/government sources, 0.1 for news, 0.0 for others
                </code>
            </div>

            <h3>üî¥ Intelligence Quality Score (NATO Intelligence Doctrine)</h3>
            <p>Strategic-grade assessment using NATO Intelligence Doctrine (NID) for analysis and reporting.</p>
            
            <h4>Overview</h4>
            <p>Our quality assessment system uses <strong>NATO Intelligence Doctrine (NID)</strong> standards for evaluating source reliability and information credibility. This military-grade methodology provides rigorous, standardized assessment criteria for intelligence evaluation.</p>
            
            <h4>Source Reliability Scale (A-F)</h4>
            <table class="confidence-table">
                <tr>
                    <th>Grade</th>
                    <th>Classification</th>
                    <th>Description</th>
                    <th>Numeric Value</th>
                </tr>
                <tr>
                    <td><strong>A</strong></td>
                    <td>Reliable</td>
                    <td>No doubt about authenticity, trustworthiness, competency</td>
                    <td>1.0</td>
                </tr>
                <tr>
                    <td><strong>B</strong></td>
                    <td>Usually reliable</td>
                    <td>Minor doubts, mostly valid information history</td>
                    <td>0.8</td>
                </tr>
                <tr>
                    <td><strong>C</strong></td>
                    <td>Fairly reliable</td>
                    <td>Some doubts, provided valid information in past</td>
                    <td>0.6</td>
                </tr>
                <tr>
                    <td><strong>D</strong></td>
                    <td>Not usually reliable</td>
                    <td>Significant doubts about reliability</td>
                    <td>0.4</td>
                </tr>
                <tr>
                    <td><strong>E</strong></td>
                    <td>Unreliable</td>
                    <td>Lacks authenticity, history of invalid information</td>
                    <td>0.2</td>
                </tr>
                <tr>
                    <td><strong>F</strong></td>
                    <td>Cannot be judged</td>
                    <td>Insufficient information to evaluate reliability</td>
                    <td>0.0</td>
                </tr>
            </table>
            
            <h4>Information Credibility Scale (1-6)</h4>
            <table class="confidence-table">
                <tr>
                    <th>Grade</th>
                    <th>Classification</th>
                    <th>Description</th>
                    <th>Numeric Value</th>
                </tr>
                <tr>
                    <td><strong>1</strong></td>
                    <td>Confirmed</td>
                    <td>Logical, consistent, confirmed by independent sources</td>
                    <td>1.0</td>
                </tr>
                <tr>
                    <td><strong>2</strong></td>
                    <td>Probably true</td>
                    <td>Logical, consistent, not confirmed</td>
                    <td>0.8</td>
                </tr>
                <tr>
                    <td><strong>3</strong></td>
                    <td>Possibly true</td>
                    <td>Reasonably logical, agrees with some information</td>
                    <td>0.6</td>
                </tr>
                <tr>
                    <td><strong>4</strong></td>
                    <td>Doubtfully true</td>
                    <td>Not logical but possible, not confirmed</td>
                    <td>0.4</td>
                </tr>
                <tr>
                    <td><strong>5</strong></td>
                    <td>Improbable</td>
                    <td>Not logical, contradicted by other information</td>
                    <td>0.2</td>
                </tr>
                <tr>
                    <td><strong>6</strong></td>
                    <td>Cannot be judged</td>
                    <td>Validity cannot be determined</td>
                    <td>0.0</td>
                </tr>
            </table>
            
            <h4>Additional Quality Factors (0.0-1.0 Scale)</h4>
            <div class="formula-box">
                <div class="formula-title">Supplementary Assessment Criteria:</div>
                <code>
                ‚Ä¢ <strong>Specificity Score:</strong> How specific and detailed are the claims?
                <br>‚Ä¢ <strong>Recency Score:</strong> How current is the information? 
                <br>‚Ä¢ <strong>Evidence Score:</strong> Are claims supported by data, studies, or examples?
                <br>‚Ä¢ <strong>Expert Score:</strong> Does the author/source have relevant credentials?
                <br><br>
                Each factor scored 0.0-1.0 based on LLM analysis with specific criteria:
                <br>‚Ä¢ 0.9-1.0: Exceptional quality
                <br>‚Ä¢ 0.7-0.89: High quality
                <br>‚Ä¢ 0.5-0.69: Moderate quality  
                <br>‚Ä¢ 0.3-0.49: Low quality
                <br>‚Ä¢ 0.0-0.29: Very poor quality
                </code>
            </div>
            
            <h4>Overall Quality Score Algorithm</h4>
            <div class="formula-box">
                <div class="formula-title">Composite Score Calculation:</div>
                <code>
                Overall_Score = (Reliability_Numeric √ó 0.3) + (Credibility_Numeric √ó 0.3) + 
                               (Evidence_Score √ó 0.2) + (Expert_Score √ó 0.1) + 
                               (Specificity_Score √ó 0.05) + (Recency_Score √ó 0.05)
                <br><br>
                Where:
                <br>‚Ä¢ Reliability_Numeric = {A: 1.0, B: 0.8, C: 0.6, D: 0.4, E: 0.2, F: 0.0}
                <br>‚Ä¢ Credibility_Numeric = {1: 1.0, 2: 0.8, 3: 0.6, 4: 0.4, 5: 0.2, 6: 0.0}
                <br>‚Ä¢ Final score rounded to 3 decimal places
                <br><br>
                <strong>Example:</strong> Source rated B2 with high evidence (0.8), moderate expert (0.6), 
                <br>good specificity (0.7), recent (0.9) = 
                <br>(0.8√ó0.3) + (0.8√ó0.3) + (0.8√ó0.2) + (0.6√ó0.1) + (0.7√ó0.05) + (0.9√ó0.05) = 0.755
                </code>
            </div>
            
            <h4>Quality Grade Distribution</h4>
            <table class="confidence-table">
                <tr>
                    <th>Overall Score Range</th>
                    <th>Quality Grade</th>
                    <th>Typical NID Rating</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td>0.85 - 1.0</td>
                    <td>Excellent</td>
                    <td>A1, A2, B1</td>
                    <td>Highest quality: Reliable sources with confirmed information</td>
                </tr>
                <tr>
                    <td>0.65 - 0.84</td>
                    <td>Good</td>
                    <td>B2, B3, C1, C2</td>
                    <td>High quality: Generally reliable with good credibility</td>
                </tr>
                <tr>
                    <td>0.45 - 0.64</td>
                    <td>Average</td>
                    <td>C3, C4, D1, D2</td>
                    <td>Moderate quality: Some reliability concerns</td>
                </tr>
                <tr>
                    <td>0.25 - 0.44</td>
                    <td>Below Average</td>
                    <td>D3, D4, E1, E2</td>
                    <td>Low quality: Significant reliability or credibility issues</td>
                </tr>
                <tr>
                    <td>0.0 - 0.24</td>
                    <td>Poor</td>
                    <td>E3-E6, F1-F6</td>
                    <td>Very poor quality: Unreliable or unverifiable</td>
                </tr>
            </table>
            
            <h4>LLM Assessment Process</h4>
            <div class="formula-box">
                <div class="formula-title">Automated Quality Evaluation Workflow:</div>
                <code>
                For each collected artifact:
                <br>1. content_analysis = LLM_evaluate(url, title, content, source_metadata)
                <br>2. source_reliability = parse_reliability_grade(analysis) # A-F
                <br>3. info_credibility = parse_credibility_grade(analysis) # 1-6  
                <br>4. additional_factors = parse_supplementary_scores(analysis) # 0.0-1.0
                <br>5. overall_score = calculate_composite_score(all_factors)
                <br>6. rationale = extract_detailed_reasoning(analysis)
                <br><br>
                LLM Model: Claude-3.5-Sonnet (primary), GPT-4 (fallback)
                <br>Temperature: 0.3 (balanced consistency and analysis quality)
                <br>Max Tokens: 1000 (sufficient for detailed assessment)
                </code>
            </div>
            
            <div class="warning-box">
                <h4>‚ö†Ô∏è NID Assessment Limitations</h4>
                <ul>
                    <li><strong>Context Dependency:</strong> Same source may receive different ratings based on specific content</li>
                    <li><strong>LLM Subjectivity:</strong> AI assessment may not capture all nuances human analysts would identify</li>
                    <li><strong>Temporal Relevance:</strong> Source reliability can change over time</li>
                    <li><strong>Domain Expertise:</strong> LLM may lack specialized cybersecurity domain knowledge</li>
                    <li><strong>Cultural Bias:</strong> NATO doctrine may not account for all cultural perspectives on credibility</li>
                </ul>
            </div>
        </div>

        <!-- Trend Analysis Methodology -->
        <div class="methodology-section" id="trend-analysis">
            <h2>üìä Comprehensive Trend Analysis</h2>
            
            <h3>Quality Trends</h3>
            <div class="formula-box">
                <div class="formula-title">Quality Improvement Calculation:</div>
                <code>
                Quality_Improvement = Recent_Average - Early_Average
                <br><br>
                Where:
                <br>‚Ä¢ Recent_Average = Mean quality score of last 3 months
                <br>‚Ä¢ Early_Average = Mean quality score of first 3 months
                <br>‚Ä¢ Trend_Direction = "improving" if Quality_Improvement > 0.05
                <br>‚Ä¢ Trend_Direction = "declining" if Quality_Improvement < -0.05
                <br>‚Ä¢ Trend_Direction = "stable" otherwise
                </code>
            </div>
            
            <h3>Topic Evolution Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Emerging/Declining Term Detection:</div>
                <code>
                Term_Frequency = Term_Count / Total_Articles_Per_Month
                <br>
                Change_Score = Recent_Period_Frequency - Early_Period_Frequency
                <br><br>
                Emerging: Change_Score > 0.01 AND Recent_Frequency > 0.01
                <br>Declining: Change_Score < -0.01 AND Early_Frequency > 0.01
                </code>
            </div>
            
            <h3>Collection Pattern Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Collection Consistency Score:</div>
                <code>
                Coefficient_of_Variation = Standard_Deviation / Mean_Daily_Collections
                <br>
                Consistency_Score = max(0, 1 - Coefficient_of_Variation)
                <br><br>
                Score Interpretation:
                <br>‚Ä¢ 1.0 = Perfect consistency
                <br>‚Ä¢ 0.8-0.99 = High consistency
                <br>‚Ä¢ 0.6-0.79 = Moderate consistency
                <br>‚Ä¢ < 0.6 = Low consistency
                </code>
            </div>
            
            <h3>Sentiment Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Keyword-Based Sentiment Classification:</div>
                <code>
                Positive_Count = Œ£(positive_keywords in text)
                <br>Negative_Count = Œ£(negative_keywords in text)
                <br>Neutral_Count = Œ£(neutral_keywords in text)
                <br><br>
                Classification Rules:
                <br>‚Ä¢ "positive" if Positive_Count > Negative_Count AND Positive_Count ‚â• Neutral_Count
                <br>‚Ä¢ "negative" if Negative_Count > Positive_Count AND Negative_Count ‚â• Neutral_Count
                <br>‚Ä¢ "neutral" otherwise
                </code>
            </div>
            
            <div class="threshold-list">
                <h4>‚ö° Analysis Thresholds</h4>
                <ul>
                    <li><strong>Minimum months for trend detection:</strong> 4 months</li>
                    <li><strong>Emergence threshold:</strong> 0.01 frequency change (0.005 for 12+ months)</li>
                    <li><strong>Quality improvement threshold:</strong> ¬±0.05 points</li>
                    <li><strong>Minimum article count for sentiment:</strong> Articles must contain sentiment keywords</li>
                </ul>
            </div>
        </div>

        <!-- Job Market Sentiment Methodology -->
        <div class="methodology-section" id="job-market-sentiment">
            <h2>üéØ Job Market Sentiment Analysis</h2>
            
            <h3>Overall Sentiment Calculation</h3>
            <div class="formula-box">
                <div class="formula-title">Sentiment Score Formula:</div>
                <code>
                Sentiment_Score = (Positive_Signals - Negative_Signals) / Total_Signals
                <br><br>
                Where:
                <br>‚Ä¢ Positive_Signals = Count of positive job-related terms
                <br>‚Ä¢ Negative_Signals = Count of negative job-related terms
                <br>‚Ä¢ Total_Signals = Positive_Signals + Negative_Signals + Neutral_Signals
                <br>‚Ä¢ Score ranges from -1.0 (very negative) to +1.0 (very positive)
                </code>
            </div>
            
            <h3>Opportunity vs. Threat Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Balance Ratio Calculation:</div>
                <code>
                Balance_Ratio = Total_Opportunities / max(Total_Threats, 1)
                <br><br>
                Narrative Classification:
                <br>‚Ä¢ "opportunity-focused" if Balance_Ratio > 1.2
                <br>‚Ä¢ "threat-focused" if Balance_Ratio < 0.8
                <br>‚Ä¢ "balanced" otherwise
                </code>
            </div>
            
            <h3>Skill Demand Analysis</h3>
            <div class="formula-title">Skill Ranking Algorithm:</div>
            <code>
            Skill_Sentiment = (Positive_Context - Negative_Context) / Total_Context_Signals
            <br>
            Overall_Rating = (Skill_Sentiment + 1) √ó Mention_Count
            <br><br>
            Where context is analyzed within ¬±200 characters of skill mention
            </code>
        </div>

        <!-- AI Adoption Predictions Methodology -->
        <div class="methodology-section" id="ai-adoption-predictions">
            <h2>üöÄ AI Adoption Rate Predictions</h2>
            
            <h3>Predictive Model Overview</h3>
            <p>Our AI adoption prediction system uses statistical analysis and pattern matching on existing processed article data to forecast cybersecurity workforce transformation trends. <strong>Note: This system does NOT use external AI models but leverages classical ML techniques on pre-processed content.</strong></p>
            
            <h3>Job Role Pattern Matching Algorithm</h3>
            <div class="formula-box">
                <div class="formula-title">Job Role Detection Formula:</div>
                <code>
                Job_Mentions = Œ£(regex_matches(pattern_i, article_text))
                <br><br>
                For each job category j:
                <br>‚Ä¢ job_articles[j] = {articles where Job_Mentions > 0}
                <br>‚Ä¢ Pattern sets defined for: security_analyst, penetration_tester, 
                <br>  security_engineer, compliance_officer, ciso_manager, ai_security_specialist
                </code>
            </div>
            
            <h3>Automation Risk Prediction</h3>
            <div class="formula-box">
                <div class="formula-title">Automation Likelihood Calculation:</div>
                <code>
                avg_replace_score = mean(replace_confidence_scores)
                <br>
                timeframe_multiplier = {6months: 1.1, 1year: 1.3, 2years: 1.8, 3years: 2.2, 5years: 3.0}
                <br>
                base_prediction = min(avg_replace_score √ó multiplier, 0.95)
                <br>
                automation_likelihood = base_prediction √ó 100
                <br><br>
                Confidence_Interval = [max(0, (base_prediction - 0.15) √ó 100), 
                <br>                     min(100, (base_prediction + 0.15) √ó 100)]
                </code>
            </div>
            
            <h3>Skills Demand Forecasting</h3>
            <div class="formula-box">
                <div class="formula-title">Skills Demand Score Calculation:</div>
                <code>
                For each skill category:
                <br>‚Ä¢ mention_count = Œ£(skill_term_occurrences in article_text)
                <br>‚Ä¢ new_tasks_confidence = mean(new_tasks_scores from articles)
                <br>‚Ä¢ augment_confidence = mean(augment_scores from articles)
                <br>‚Ä¢ demand_score = (new_tasks_confidence + augment_confidence) / 2
                <br>‚Ä¢ growth_projection = demand_score √ó timeframe_multiplier
                </code>
            </div>
            
            <h3>Industry Adoption Velocity</h3>
            <div class="formula-box">
                <div class="formula-title">Adoption Rate Calculation:</div>
                <code>
                sector_articles = articles_mentioning(sector_keywords)
                <br>
                early_period_confidence = mean(confidence_scores[first_3_months])
                <br>recent_period_confidence = mean(confidence_scores[last_3_months])
                <br>
                adoption_velocity = (recent_period_confidence - early_period_confidence) / time_span
                <br>projected_adoption = current_confidence + (adoption_velocity √ó timeframe_months)
                </code>
            </div>
            
            <h3>Statistical Confidence Metrics</h3>
            <table class="confidence-table">
                <tr>
                    <th>Metric</th>
                    <th>Calculation Method</th>
                    <th>Reliability Threshold</th>
                </tr>
                <tr>
                    <td>Evidence Articles</td>
                    <td>Count of articles containing job/skill patterns</td>
                    <td>Minimum 10 articles for reliable prediction</td>
                </tr>
                <tr>
                    <td>Model Confidence</td>
                    <td>Fixed at 0.85 based on validation testing</td>
                    <td>Predictions with &lt;0.7 confidence flagged</td>
                </tr>
                <tr>
                    <td>Confidence Interval</td>
                    <td>¬±15% around base prediction</td>
                    <td>Wider intervals for sparse data</td>
                </tr>
            </table>
            
            <h3>Technology Impact Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Technology Disruption Score:</div>
                <code>
                tech_mentions = Œ£(technology_pattern_matches)
                <br>replace_impact = mean(replace_confidence √ó tech_mention_density)
                <br>augment_potential = mean(augment_confidence √ó tech_mention_density)
                <br>
                disruption_score = (replace_impact √ó 0.7) + (augment_potential √ó 0.3)
                <br>projected_impact = disruption_score √ó timeframe_multiplier
                </code>
            </div>
            
            <div class="warning-box">
                <h4>‚ö†Ô∏è Predictive Model Limitations</h4>
                <ul>
                    <li><strong>Pattern-Based:</strong> Uses regex pattern matching, not semantic understanding</li>
                    <li><strong>Historical Bias:</strong> Predictions based solely on existing article content</li>
                    <li><strong>Fixed Multipliers:</strong> Timeframe multipliers are empirically derived, not dynamically calculated</li>
                    <li><strong>Confidence Intervals:</strong> Fixed ¬±15% range may not reflect true uncertainty</li>
                    <li><strong>Sample Dependency:</strong> Quality depends on diversity and volume of input articles</li>
                </ul>
            </div>
            
            <h3>Validation Methodology</h3>
            <p>Prediction accuracy is validated through:</p>
            <ul>
                <li>Cross-validation with external industry reports</li>
                <li>Historical back-testing on previous data periods</li>
                <li>Comparison with expert assessments</li>
                <li>Regular recalibration of timeframe multipliers</li>
            </ul>
        </div>

        <!-- Collection Monitoring Methodology -->
        <div class="methodology-section" id="collection-monitoring">
            <h2>üîç Collection Monitoring Dashboard</h2>
            
            <h3>Collection Velocity Tracking</h3>
            <div class="formula-box">
                <div class="formula-title">Velocity Metrics:</div>
                <code>
                Collection_Rate = Articles_Collected / Time_Period_Hours
                <br>
                Peak_Hour_Detection = max(hourly_collection_counts)
                <br>
                Efficiency_Score = Successful_Collections / Total_Attempts
                </code>
            </div>
            
            <h3>Source Health Assessment</h3>
            <div class="formula-box">
                <div class="formula-title">Source Health Score:</div>
                <code>
                Health_Score = (Quality_Score √ó 0.4) + (Reliability_Score √ó 0.3) + (Freshness_Score √ó 0.3)
                <br><br>
                Where:
                <br>‚Ä¢ Quality_Score = Average quality of articles from source
                <br>‚Ä¢ Reliability_Score = Success rate of collections
                <br>‚Ä¢ Freshness_Score = Recency of last successful collection
                </code>
            </div>
            
            <h3>Anomaly Detection</h3>
            <div class="formula-box">
                <div class="formula-title">Anomaly Thresholds:</div>
                <code>
                Quality_Anomaly = |current_quality - rolling_average| > 2 √ó standard_deviation
                <br>
                Volume_Anomaly = |current_volume - expected_volume| > 3 √ó standard_deviation
                <br>
                Source_Issue = consecutive_failures > 3 OR response_time > 30_seconds
                </code>
            </div>
        </div>

        <!-- Category Distribution Methodology -->
        <div class="methodology-section" id="category-distribution">
            <h2>üìä AI Impact Category Classification</h2>
            
            <h3>LLM-Based Classification System</h3>
            <p>Articles are classified into four primary AI impact categories using Large Language Model (LLM) analysis with confidence scoring. <strong>This is the core intelligence layer that powers all downstream analysis.</strong></p>
            
            <div class="formula-box">
                <div class="formula-title">Classification Categories & Definitions:</div>
                <code>
                AI_REPLACE: Articles discussing AI replacing human tasks entirely
                <br>‚Ä¢ Indicators: "automate", "replace", "eliminate jobs", "reduce headcount"
                <br><br>
                AI_AUGMENT: Articles about AI enhancing human capabilities  
                <br>‚Ä¢ Indicators: "assist", "enhance", "support", "collaboration"
                <br><br>
                NEW_TASKS: Articles describing new roles/tasks created by AI
                <br>‚Ä¢ Indicators: "new role", "emerging skill", "AI specialist", "prompt engineer"
                <br><br>
                HUMAN_ONLY: Articles emphasizing tasks requiring uniquely human skills
                <br>‚Ä¢ Indicators: "human judgment", "creativity", "empathy", "strategic thinking"
                </code>
            </div>
            
            <h3>LLM Analysis Process</h3>
            <div class="formula-box">
                <div class="formula-title">Multi-Category Classification Algorithm:</div>
                <code>
                For each article:
                <br>1. content_analysis = LLM_analyze(article_text, category_definitions)
                <br>2. confidence_scores = extract_confidence(content_analysis)
                <br>3. reasoning = extract_reasoning(content_analysis)
                <br><br>
                Output Format:
                <br>{
                <br>  "replace": {"confidence": 0.0-1.0, "reasoning": "..."},
                <br>  "augment": {"confidence": 0.0-1.0, "reasoning": "..."},
                <br>  "new_tasks": {"confidence": 0.0-1.0, "reasoning": "..."},
                <br>  "human_only": {"confidence": 0.0-1.0, "reasoning": "..."}
                <br>}
                </code>
            </div>
            
            <h3>Confidence Score Calibration</h3>
            <div class="formula-box">
                <div class="formula-title">Confidence Thresholds & Validation:</div>
                <code>
                High Confidence: score ‚â• 0.7 (Strong evidence in article)
                <br>Medium Confidence: 0.4 ‚â§ score < 0.7 (Some evidence present)
                <br>Low Confidence: score < 0.4 (Minimal or unclear evidence)
                <br><br>
                Validation Rules:
                <br>‚Ä¢ Total confidence across categories can exceed 1.0 (articles may fit multiple categories)
                <br>‚Ä¢ Minimum 0.1 confidence required for category assignment
                <br>‚Ä¢ Manual spot-checking performed on 5% of classifications
                </code>
            </div>
            
            <h3>Model Configuration</h3>
            <table class="confidence-table">
                <tr>
                    <th>Parameter</th>
                    <th>Value</th>
                    <th>Purpose</th>
                </tr>
                <tr>
                    <td>Primary Model</td>
                    <td>Claude-3.5-Sonnet-20241022</td>
                    <td>Main classification engine</td>
                </tr>
                <tr>
                    <td>Fallback Model</td>
                    <td>GPT-4</td>
                    <td>Backup for API failures</td>
                </tr>
                <tr>
                    <td>Temperature</td>
                    <td>0.1</td>
                    <td>Low randomness for consistent results</td>
                </tr>
                <tr>
                    <td>Max Tokens</td>
                    <td>1500</td>
                    <td>Sufficient for detailed reasoning</td>
                </tr>
                <tr>
                    <td>Retry Logic</td>
                    <td>3 attempts with exponential backoff</td>
                    <td>Handle API rate limits</td>
                </tr>
            </table>
            
            <h3>Category Distribution Analysis</h3>
            <div class="formula-box">
                <div class="formula-title">Distribution Balance Score:</div>
                <code>
                For each category i, calculate proportion:
                <br>p[i] = count(articles_with_category[i]) / total_articles
                <br><br>
                Gini_Coefficient = 1 - Œ£(p[i]¬≤)
                <br>Balance_Score = Gini_Coefficient / max_possible_gini
                <br><br>
                Perfect balance (0.25 each) = Score of 1.0
                <br>Complete imbalance (100% one category) = Score of 0.0
                </code>
            </div>
            
            <div class="warning-box">
                <h4>‚ö†Ô∏è Classification Limitations</h4>
                <ul>
                    <li><strong>Model Bias:</strong> LLM training data may introduce systematic biases</li>
                    <li><strong>Context Sensitivity:</strong> Same content may be interpreted differently based on surrounding text</li>
                    <li><strong>Confidence Calibration:</strong> LLM confidence may not reflect true uncertainty</li>
                    <li><strong>Cost Constraints:</strong> API costs limit real-time re-classification</li>
                    <li><strong>Version Dependency:</strong> Model updates may change classification behavior</li>
                </ul>
            </div>
            
            <h3>Quality Assurance Process</h3>
            <ul>
                <li><strong>Consistency Checks:</strong> Same article re-classified should yield similar results</li>
                <li><strong>Cross-Model Validation:</strong> Occasional validation using multiple LLM providers</li>
                <li><strong>Human Review:</strong> Expert review of edge cases and high-impact classifications</li>
                <li><strong>Confidence Monitoring:</strong> Track confidence distributions for anomaly detection</li>
                <li><strong>Error Analysis:</strong> Regular analysis of classification failures and edge cases</li>
            </ul>
        </div>

        <!-- Confidence and Limitations -->
        <div class="methodology-section" id="confidence-limitations">
            <h2>‚ö†Ô∏è Confidence Levels and Limitations</h2>
            
            <h3>Confidence Assessment</h3>
            <table class="confidence-table">
                <tr>
                    <th>Confidence Level</th>
                    <th>Data Requirements</th>
                    <th>Reliability</th>
                </tr>
                <tr>
                    <td>High</td>
                    <td>6+ months, 100+ articles</td>
                    <td>Results are statistically significant</td>
                </tr>
                <tr>
                    <td>Medium</td>
                    <td>3-5 months, 50+ articles</td>
                    <td>Trends are indicative but may change</td>
                </tr>
                <tr>
                    <td>Low</td>
                    <td>< 3 months, < 50 articles</td>
                    <td>Preliminary insights only</td>
                </tr>
            </table>
            
            <div class="warning-box">
                <h4>‚ö†Ô∏è Important Limitations</h4>
                <ul>
                    <li><strong>Data Bias:</strong> Results reflect available sources and may not represent all perspectives</li>
                    <li><strong>Temporal Sensitivity:</strong> Analysis is based on historical data and may not predict future changes</li>
                    <li><strong>Keyword Limitations:</strong> Sentiment analysis is keyword-based and may miss nuanced context</li>
                    <li><strong>Sample Size:</strong> Small datasets may produce unreliable results</li>
                    <li><strong>Source Quality:</strong> Analysis quality depends on the quality of collected sources</li>
                </ul>
            </div>
            
            <h3>Best Practices for Interpretation</h3>
            <ul>
                <li>Always consider confidence levels when interpreting results</li>
                <li>Look for consistent patterns across multiple analysis types</li>
                <li>Combine quantitative results with qualitative assessment</li>
                <li>Regular re-analysis as new data becomes available</li>
                <li>Cross-reference with external industry reports</li>
            </ul>
        </div>

        <!-- Technical Implementation -->
        <div class="methodology-section" id="technical-implementation">
            <h2>üîß Technical Implementation</h2>
            
            <h3>Data Processing Pipeline</h3>
            <ol>
                <li><strong>Collection:</strong> Automated gathering from configured sources</li>
                <li><strong>Preprocessing:</strong> Text cleaning, normalization, metadata extraction</li>
                <li><strong>Classification:</strong> AI impact category assignment using NLP</li>
                <li><strong>Quality Scoring:</strong> Multi-factor quality assessment</li>
                <li><strong>Analysis:</strong> Statistical and machine learning analysis</li>
                <li><strong>Reporting:</strong> Automated report generation with confidence metrics</li>
            </ol>
            
            <h3>Quality Assurance</h3>
            <ul>
                <li>Automated validation of analysis results</li>
                <li>Statistical significance testing</li>
                <li>Cross-validation of machine learning models</li>
                <li>Regular methodology review and updates</li>
            </ul>
            
            <h3>Update Frequency</h3>
            <ul>
                <li><strong>Real-time:</strong> Collection monitoring and health metrics</li>
                <li><strong>Daily:</strong> Quality scores and basic statistics</li>
                <li><strong>Weekly:</strong> Trend analysis and sentiment tracking</li>
                <li><strong>Monthly:</strong> Comprehensive predictive analysis and reports</li>
            </ul>
        </div>

        <div class="navigation-links">
            <a href="/" class="nav-button">üè† Return Home</a>
            <a href="/analysis" class="nav-button">üìä Run Analysis</a>
            <a href="/summaries" class="nav-button">üìã View Reports</a>
        </div>
        
        <div style="text-align: center; margin-top: 40px; color: #7f8c8d;">
            <p><em>AI-Horizon Methodology Documentation v2.0</em></p>
            <p>Last Updated: {{ current_date }}</p>
        </div>
    </div>
{% endblock %}
